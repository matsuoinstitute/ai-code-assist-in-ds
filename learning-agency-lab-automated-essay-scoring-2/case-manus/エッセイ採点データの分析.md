# エッセイ採点データの分析

## データ構造

### 訓練データ (train.csv)
- 行数: 17,307件
- カラム: essay_id, full_text, score
- スコア分布:
  - スコア1: 1,252件
  - スコア2: 4,723件
  - スコア3: 6,280件
  - スコア4: 3,926件
  - スコア5: 970件
  - スコア6: 156件
- 欠損値: なし
- エッセイ長の統計:
  - 平均: 2,071.6文字
  - 標準偏差: 925.9文字
  - 最小: 712文字
  - 中央値: 1,924文字
  - 最大: 20,459文字

### テストデータ (test.csv)
- 行数: 3件
- カラム: essay_id, full_text (scoreカラムなし)
- 欠損値: なし
- エッセイ長の統計:
  - 平均: 2,474.3文字
  - 標準偏差: 725.5文字
  - 最小: 1,669文字
  - 中央値: 2,677文字
  - 最大: 3,077文字

## 分析結果と前処理方針

訓練データとテストデータの分析から、以下の特徴と前処理方針が考えられます。

1. スコア分布は1〜6の範囲で、中央値は3です。スコア3が最も多く、次いでスコア2、スコア4と続きます。スコア5と6は比較的少数です。このスコア分布の偏りを考慮したモデル訓練が必要です。

2. エッセイの長さは平均約2,000文字程度で、訓練データとテストデータで大きな差はありません。ただし、訓練データには最大20,459文字と非常に長いエッセイも含まれています。エッセイの長さも特徴量として考慮すべきでしょう。

3. データに欠損値はなく、前処理の観点からはクリーンな状態です。

## 前処理と特徴量設計の方針

1. テキスト前処理:
   - 小文字化
   - 句読点の処理
   - ストップワードの除去
   - スペルチェック/修正（エッセイに誤字脱字が多い可能性）
   - トークン化

2. 特徴量設計:
   - TF-IDF特徴量
   - 単語埋め込み（Word Embeddings）
   - 文章の長さ、単語数、文の数などの統計的特徴
   - 文法エラーの数、スペルミスの数などの言語品質指標
   - 語彙の多様性指標

3. モデル候補:
   - 線形回帰/ロジスティック回帰
   - ランダムフォレスト
   - 勾配ブースティング（XGBoost, LightGBM）
   - ニューラルネットワーク（特に自然言語処理向けモデル）
   - BERT等の事前学習済み言語モデルのファインチューニング
